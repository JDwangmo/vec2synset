{"name":"vec2synset","tagline":"Making word sense embeddings interpretable. A tool for matching word sense embeddings with synsets of lexical resources.","body":"### Motivation\r\n\r\nWord sense embeddings represent a word sense as a low-dimensional vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets show precision of 0.87, recall of 0.41 and AUC of 0.79.\r\n\r\nTwo key approaches to modelling semantics of lexical units are lexicography and statistical corpus analysis. In the first approach, a human explicitly encodes lexical-semantic knowledge, usually in the form of synsets, typed relations between synsets and sense definitions. A prominent examples of this approach is Princeton WordNet. The second approach makes use of text corpora to extract relations between words and feature representations of words and senses. These methods are trying to avoid manual work as much as possible. Prominent examples of this group of methods are distributional models and word embeddings.   \r\n\r\nThe strongest side of lexical-semantic resources is their interpretability -- they are entirely human-readable and drawn distinctions are motivated by lexicographic or psychological considerations. On the downside,  these WordNet-like resources are expensive to create, and it is not easy to adapt them to a given domain of interest. On the other hand, corpus-driven approaches are strong at adaptivity -- they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses. However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, especially for dense vector representations as present in latent vector spaces  and word embeddings, the representations are barely interpretable. \r\n\r\n\r\nThe main motivation of the technique described in this paper is to close this gap between interpretability and adaptivity of lexical-semantic models by linking word sense embeddings to synsets. We demonstrate usage of our method on a combination of BabelNet and AdaGram  sense embeddings. However the approach can be straightforwardly applied to any WordNet-like resource and word sense embeddings.  \r\n\r\n### Downloads \r\n\r\n* [AdaGram word sense embeddings trained on the ukWaC+WaCypedia_EN corpus](http://panchenko.me/data/joint/adagram-model-wacky+ukwac-raw). Use [AdaGram](github.com/sbos/AdaGram.jl) to load these word sense vectors.\r\n* You can obtain BabelNet synsets from their [official site](http://babelnet.org)\r\n* [Mapping between AdaGram sense embeddings and BabelNet synsets for 5209 ambigous words](https://docs.google.com/spreadsheets/d/1DvPAgndoYg15GhElDqWLz4fySoX3GISqabMrLTABCtU/edit?usp=sharing)\r\n* [Evaluation dataset: manual mapping between AdaGram sense embeddings and BabelNet senses for 50 words](https://docs.google.com/spreadsheets/d/1h_R0Ja1sZnFBud0nPPWb9JO6kB_8NfDz76AISo9FWXY/edit?usp=sharing)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}