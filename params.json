{
  "name": "vec2synset",
  "tagline": "Making word sense embeddings interpretable: a tool for tagging word sense embeddings with synsets of lexical resources.",
  "body": "This page contains implementation of a method for mapping word sense embeddings to synsets, descibed in the paper [\"Best of Both Worlds: Making Word Sense Embeddings Interpretable\"](http://www.lrec-conf.org/proceedings/lrec2016/pdf/625_Paper.pdf). Word sense embeddings represent a word sense as a low-dimensional numeric vector. While this representation is potentially useful for NLP applications, its interpretability is inherently limited. We propose a simple technique that improves interpretability of sense vectors by mapping them to synsets of a lexical resource. Our experiments with AdaGram sense embeddings and BabelNet synsets show that it is possible to retrieve synsets that correspond to automatically learned sense vectors with Precision of 0.87, Recall of 0.42 and AUC of 0.78. \r\n\r\n## Motivation: Two Worlds of Lexical Semantics\r\n\r\n\r\nTwo key approaches to modelling semantics of lexical units are lexicography and statistical corpus analysis. In the first approach, a human explicitly encodes lexical-semantic knowledge, usually in the form of synsets, typed relations between synsets and sense definitions. A prominent examples of this approach is Princeton WordNet. The second approach makes use of text corpora to extract relations between words and feature representations of words and senses. These methods are trying to avoid manual work as much as possible. Prominent examples of this group of methods are distributional models and word embeddings.   \r\n\r\nThe strongest side of lexical-semantic resources is their interpretability -- they are entirely human-readable and drawn distinctions are motivated by lexicographic or psychological considerations. On the downside,  these WordNet-like resources are expensive to create, and it is not easy to adapt them to a given domain of interest. On the other hand, corpus-driven approaches are strong at adaptivity -- they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses. However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, especially for dense vector representations as present in latent vector spaces  and word embeddings, the representations are barely interpretable. \r\n\r\n\r\nThe main motivation of our method is to close this gap between interpretability and adaptivity of lexical-semantic models by linking word sense embeddings to synsets. We demonstrate usage of our method on a combination of BabelNet and AdaGram  sense embeddings. However the approach can be straightforwardly applied to any WordNet-like resource and word sense embeddings.  \r\n\r\n\r\n\r\n## Method: Linking Embeddings to Synsets\r\n\r\nOur matching technique takes as input a trained word sense embeddings model, a set of synsets from a lexical resource and outputs a mapping from sense embeddings to synsets of the lexical resource. The method includes four steps. First, we convert word sense embeddings to a lexicalized representation and perform alignment via word overlap.  Second we build a bag-of-word (BoW) representation of synsets. Third, we build a bag-of-word representation of  sense embeddings. Finally, we measure similarity of senses and link most similar vector-synset pairs. The following figure desribes overview of the overall procedure:\r\n\r\n![Schema](https://raw.githubusercontent.com/tudarmstadt-lt/vec2synset/master/data/schema.png \"Schema\")\r\n\r\n\r\n## Citation Information\r\n\r\nIf you would like to refer to the vec2synset approach please use this citation ([PDF](http://www.lrec-conf.org/proceedings/lrec2016/pdf/625_Paper.pdf)):\r\n\r\n```\r\n@InProceedings{PANCHENKO16.625,\r\n  author = {Alexander Panchenko},\r\n  title = {Best of Both Worlds: Making Word Sense Embeddings Interpretable},\r\n  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},\r\n  year = {2016},\r\n  month = {may},\r\n  date = {23-28},\r\n  location = {Portoro≈æ, Slovenia},\r\n  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Asuncion Moreno and Jan Odijk and Stelios Piperidis},\r\n  publisher = {European Language Resources Association (ELRA)},\r\n  address = {Paris, France},\r\n  isbn = {978-2-9517408-9-1},\r\n }\r\n```\r\n\r\n\r\n\r\n## Downloads: Reproducing Results \r\n\r\n* [AdaGram word sense embeddings](http://panchenko.me/data/joint/adagram/adagram-model-wacky+ukwac-raw) trained on the combination of the ukWaC and [WaCypedia_EN](http://wacky.sslmit.unibo.it/doku.php?id=corpora) corpora. Use [AdaGram](https://github.com/sbos/AdaGram.jl) tool to load and use these word sense vectors.\r\n* You can obtain BabelNet synsets from their [official site](http://babelnet.org). In the experiment, described in the paper we used synsets and sense embeddings of the following [5209 ambiguous words](http://panchenko.me/data/joint/adagram/voc-5209.csv). \r\n* [Mapping](https://docs.google.com/spreadsheets/d/1DvPAgndoYg15GhElDqWLz4fySoX3GISqabMrLTABCtU/edit?usp=sharing) between AdaGram sense embeddings and BabelNet synsets for the mentioned above 5209 ambigous words.\r\n* [Evaluation dataset](https://docs.google.com/spreadsheets/d/1h_R0Ja1sZnFBud0nPPWb9JO6kB_8NfDz76AISo9FWXY/edit?usp=sharing) manual mapping between AdaGram sense embeddings and BabelNet synsets for ambigous 50 words.\r\n* [More AdaGram sense embeddings](http://panchenko.me/data/joint/adagram/) based on different sense granularities (the alpha parameter 0.05, 0.12, 0.15, 0.20, 0.75) trained on a 59 Gb corpus (a combination of Wikipedia, ukWaC, Gigaword and Leipzig news corpus).",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}